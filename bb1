import os
import ssl
import urllib3
import uuid
import requests
import time
import datetime
from requests.auth import HTTPBasicAuth

# ========================= CORPORATE PROXY CONFIGURATION =========================
def setup_corporate_proxy():
    """Complete setup for corporate proxy with SSL bypass"""
    
    # 1. Set proxy environment variables
    proxy_url = 'http://webproxy.bfm.com:8080'
    os.environ['HTTP_PROXY'] = proxy_url
    os.environ['HTTPS_PROXY'] = proxy_url
    os.environ['http_proxy'] = proxy_url
    os.environ['https_proxy'] = proxy_url
    
    # 2. Disable SSL verification globally
    ssl._create_default_https_context = ssl._create_unverified_context
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    # 3. Clear certificate bundles
    os.environ['CURL_CA_BUNDLE'] = ''
    os.environ['REQUESTS_CA_BUNDLE'] = ''
    os.environ['SSL_VERIFY'] = 'false'
    os.environ['PYTHONHTTPSVERIFY'] = '0'
    
    print(f"âœ… Corporate proxy configured: {proxy_url}")

# Apply configuration before any imports
setup_corporate_proxy()

# ========================= IMPORTS =========================
from skywalker import get_and_save_gdelt_articles
from rank_bm25 import BM25Okapi
import pandas as pd
import json
from typing import Dict, List, Tuple, Optional, Any
import logging
from fuzzywuzzy import fuzz
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# Configure logging to reduce verbosity
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING)

# ========================= BLACKROCK API CONFIGURATION =========================
# Authentication credentials
BLK_USERNAME = os.environ.get("BLK_USERNAME", "dshad")
BLK_PASSWORD = os.environ.get("BLK_PASSWORD", "dasdasd")
BLK_CLIENT_ENV = os.environ.get("BLK_CLIENT_ENV", "dasd")
BLK_API_KEY = os.environ.get("BLK_API_KEY", "dasd")

# API Endpoints
BLK_COMPUTE_ASYNC_URL = f"https://{BLK_CLIENT_ENV}.blackrock.com/api/ai-platform/toolkit/chat-completion/v1/chatCompletions:compute"
BLK_LONG_RUNNING_URL = f"https://{BLK_CLIENT_ENV}.blackrock.com/api/ai-platform/toolkit/chat-completion/v1/longRunningOperations"

# ========================= BLACKROCK LLM CLIENT =========================

class BlackRockLLMClient:
    """LLM client that uses BlackRock's chat completion API"""
    
    def __init__(self, model_id: str = "gpt-4o"):
        self.model_id = model_id
        self.username = BLK_USERNAME
        self.password = BLK_PASSWORD
        self.api_key = BLK_API_KEY
        self.compute_url = BLK_COMPUTE_ASYNC_URL
        self.operations_url = BLK_LONG_RUNNING_URL
        self.provider = "blackrock"
        self.model = model_id
        print(f"ðŸ¤– Initialized BlackRock LLM client with model: {self.model_id}")
    
    def _generate_headers(self) -> Dict[str, str]:
        """Generate required headers for API requests."""
        return {
            'VND.com.blackrock.API-Key': self.api_key,
            'VND.com.blackrock.Origin-Timestamp': str(datetime.datetime.utcnow().replace(microsecond=0).astimezone().isoformat()),
            'VND.com.blackrock.Request-ID': str(uuid.uuid1())
        }
    
    def _execute_async_request(self, messages: List[Dict], max_tokens: int = 3000, temperature: float = 0.1) -> Dict:
        """Execute asynchronous chat completion request and wait for results."""
        # Build request payload
        request_payload = {
            "chatCompletionMessages": messages,
            "modelId": self.model_id,
            "modelParam": {
                "openAI": {
                    "frequencyPenalty": 0.0,
                    "temperature": temperature,
                    "maxTokens": max_tokens
                }
            }
        }
        
        try:
            # Start asynchronous request
            response_post = requests.post(
                self.compute_url,
                auth=HTTPBasicAuth(self.username, self.password),
                headers=self._generate_headers(),
                json=request_payload,
                verify=False  # For corporate proxy
            )
            
            if response_post.status_code != 200:
                raise Exception(f"API request failed with status {response_post.status_code}: {response_post.text}")
            
            # Extract operation ID
            response_json = response_post.json()
            operation_id = response_json.get('id')
            if not operation_id:
                raise Exception(f"No operation ID in response: {response_json}")
            
            operation_url = f"{self.operations_url}/{operation_id}"
            
            # Wait for results with retries
            max_retries = 15
            wait_time = 2
            
            for retry_num in range(max_retries):
                result_response = requests.get(
                    operation_url,
                    auth=HTTPBasicAuth(self.username, self.password),
                    headers=self._generate_headers(),
                    verify=False  # For corporate proxy
                )
                
                if result_response.status_code != 200:
                    raise Exception(f"Status check failed with status {result_response.status_code}: {result_response.text}")
                
                result_json = result_response.json()
                
                if result_json.get('done'):
                    return result_json
                
                if retry_num < max_retries - 1:
                    time.sleep(wait_time)
            
            raise Exception("Request timed out after maximum retries")
            
        except Exception as e:
            raise Exception(f"BlackRock API error: {str(e)}")
    
    def generate(self, prompt: str, system_prompt: str = "", temperature: float = 0.1, 
                max_tokens: int = 3000, response_format: Optional[Dict] = None) -> str:
        """Generate text using BlackRock API"""
        try:
            # Build messages
            messages = []
            if system_prompt:
                messages.append({"prompt": system_prompt, "promptRole": "system"})
            messages.append({"prompt": prompt, "promptRole": "user"})
            
            # Add JSON instruction if response format is specified
            if response_format and response_format.get("type") == "json_object":
                messages[-1]["prompt"] += "\n\nPlease respond with valid JSON only."
            
            # Execute request
            result = self._execute_async_request(messages, max_tokens, temperature)
            
            # Extract response
            if result.get('response') and 'chatCompletion' in result['response']:
                chat_completion = result['response']['chatCompletion']
                if 'chatCompletionContent' in chat_completion:
                    return chat_completion['chatCompletionContent'].strip()
                else:
                    raise Exception("No chatCompletionContent in response")
            else:
                raise Exception("Unexpected response structure")
                
        except Exception as e:
            print(f"âŒ BlackRock API error: {e}")
            return f"Error: {str(e)}"
    
    def generate_structured(self, prompt: str, schema: Dict, system_prompt: str = "", 
                          temperature: float = 0.1, max_tokens: int = 3000) -> Dict:
        """Generate structured output with schema validation"""
        try:
            # Add schema information to prompt
            schema_json = json.dumps(schema, indent=2)
            structured_prompt = f"{prompt}\n\nPlease respond with valid JSON that matches this schema:\n{schema_json}\n\nRespond with JSON only, no additional text."
            
            # Generate response
            response_text = self.generate(structured_prompt, system_prompt, temperature, max_tokens, 
                                        response_format={"type": "json_object"})
            
            if "Error:" in response_text:
                return {}
            
            # Try to parse JSON
            try:
                # Find JSON in response
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
                else:
                    # Try parsing the whole response
                    return json.loads(response_text)
            except json.JSONDecodeError:
                print(f"Failed to parse JSON response: {response_text[:200]}...")
                return {}
                
        except Exception as e:
            print(f"âŒ Error generating structured output: {e}")
            return {}

# Create a GenericLLMClient alias for backward compatibility
GenericLLMClient = BlackRockLLMClient

def test_api_with_proxy(llm_client: BlackRockLLMClient):
    """Test API calls through proxy"""
    print(f"ðŸ§ª Testing BlackRock API through corporate proxy...")
    
    try:
        test_prompt = "Hello, this is a test message. Please respond briefly."
        response = llm_client.generate(test_prompt, max_tokens=50)
        
        if "Error" not in response:
            print(f"âœ… BlackRock API working through corporate proxy!")
            print(f"   Response preview: {response[:50]}...")
            return True
        else:
            print(f"âŒ BlackRock API failed: {response}")
            return False
    except Exception as e:
        print(f"âŒ API test failed: {e}")
        return False

# Initialize the LLM client with BlackRock API
print("\nðŸš€ Initializing BlackRock LLM client with corporate proxy support...")
llm_client = BlackRockLLMClient("gpt-4o")  # Using GPT-4o model

# Test proxy and API connectivity
print("\nðŸ“¡ Testing connectivity...")
proxy_ok = test_proxy_connection()
api_ok = test_api_with_proxy(llm_client)

if proxy_ok and api_ok:
    print("\nðŸŽ‰ All systems ready with BlackRock API and corporate proxy support!")
else:
    print("\nâš ï¸ Some connectivity issues detected but continuing...")

# ========================= REST OF YOUR ORIGINAL CODE CONTINUES BELOW =========================

def test_proxy_connection():
    """Test if proxy connection is working"""
    proxy_url = os.environ.get('HTTPS_PROXY', os.environ.get('HTTP_PROXY'))
    
    if not proxy_url:
        print("âŒ No proxy configured")
        return False
    
    try:
        import requests
        
        print(f"ðŸ” Testing proxy connection: {proxy_url}")
        
        response = requests.get(
            'https://httpbin.org/ip', 
            timeout=10,
            verify=False  # Corporate proxy SSL bypass
        )
        
        if response.status_code == 200:
            result = response.json()
            print(f"âœ… Proxy working! External IP: {result.get('origin', 'Unknown')}")
            return True
        else:
            print(f"âŒ Proxy test failed with status: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Proxy test failed: {e}")
        return False

def get_sentence_model():
    """Get or initialize the sentence transformer model"""
    global sentence_model
    if 'sentence_model' not in globals():
        # Configure for proxy environment if needed
        os.environ['HF_HUB_OFFLINE'] = '0'  # Allow online downloads through proxy
        sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
    return sentence_model

def calculate_semantic_similarity_efficient(df: pd.DataFrame, query: str) -> pd.DataFrame:
    """Calculate semantic similarity using sentence transformers"""
    if df.empty:
        return df
    
    df_copy = df.copy()
    model = get_sentence_model()
    
    # Use english_title if available, otherwise use title
    title_col = 'english_title' if 'english_title' in df_copy.columns else 'title'
    titles = [t if pd.notna(t) else "" for t in df_copy[title_col]]
    
    if not any(titles):
        df_copy['semantic_similarity'] = 0.0
        return df_copy
    
    all_texts = [query] + titles
    print(f"  Computing semantic embeddings for {len(titles)} articles...")
    
    embeddings = model.encode(
        all_texts, 
        convert_to_tensor=False, 
        show_progress_bar=False,
        batch_size=32
    )
    
    sim = cosine_similarity([embeddings[0]], embeddings[1:]).flatten()
    df_copy['semantic_similarity'] = np.round(sim, 4)
    return df_copy
    
def extract_knowledge_graph_with_llm(query: str) -> Dict:
    """Extract entities, nodes, and generate search terms using BlackRock LLM"""
    kg_schema = {
        "type": "object",
        "properties": {
            "entities": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {"text": {"type": "string"}, "type": {"type": "string"}},
                    "required": ["text", "type"],
                    "additionalProperties": False
                }
            },
            "action_nodes": {"type": "array", "items": {"type": "string"}},
            "search_terms_with_entity": {"type": "array", "items": {"type": "string"}},
            "search_terms_without_entity": {"type": "array", "items": {"type": "string"}},
            "related_concepts": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["entities", "action_nodes", "search_terms_with_entity", "search_terms_without_entity", "related_concepts"],
        "additionalProperties": False
    }

    prompt = f"""
    Extract knowledge graph components from: "{query}"

    ENTITIES: Identify companies, people, organizations with exact names and types:
    - Format: {{"text": "exact name", "type": "ORG|PERSON|GPE"}}
    
    ACTION_NODES: Core concepts/events (3-5 key terms):
    - Focus: funding, investment, product launch, acquisition, etc.
    
    SEARCH_TERMS_WITH_ENTITY: Generate 15-20 entity-focused search terms:
    - ALL TERMS MUST START WITH THE MAIN ENTITY NAME
    - Examples: "Harvard endowment performance", "Harvard investment returns"
    
    SEARCH_TERMS_WITHOUT_ENTITY: Generate 10-15 broader search keywords (backup terms):
    - Industry terms: "university endowment", "endowment performance"
    
    RELATED_CONCEPTS: Broader industry context (5-8 terms):
    - ALL TERMS MUST START WITH THE MAIN ENTITY NAME
    
    CRITICAL: The main entity name should appear at the beginning of MOST search terms.
    """

    system_prompt = "You are a financial news analyst expert at extracting structured information for targeted news search. Ensure all search terms include the main entity name for precision."

    try:
        # Use the new structured generation method
        result = llm_client.generate_structured(prompt, kg_schema, system_prompt, temperature=0.1)
        return result if result else {"entities": [], "action_nodes": [], "search_terms_with_entity": [], "search_terms_without_entity": [], "related_concepts": []}
            
    except Exception as e:
        print(f"Error extracting knowledge graph: {e}")
        return {"entities": [], "action_nodes": [], "search_terms_with_entity": [], "search_terms_without_entity": [], "related_concepts": []}

def build_search_term_list(kg: Dict) -> List[str]:
    """Build comprehensive search term list from knowledge graph - prioritizing entity-focused terms"""
    terms = []
    
    # Get main entity name for validation
    main_entity = kg['entities'][0]['text'] if kg.get('entities') else ""
    
    # Add main entity first
    if main_entity:
        terms.append(main_entity)
    
    # Prioritize search terms WITH entity (these should all contain the entity name)
    entity_terms = kg.get('search_terms_with_entity', [])
    if main_entity:
        # Validate that entity terms actually contain the entity name
        validated_entity_terms = []
        for term in entity_terms:
            if main_entity.lower() in term.lower():
                validated_entity_terms.append(term)
            else:
                # If term doesn't contain entity, prepend it
                validated_entity_terms.append(f"{main_entity} {term}")
        terms.extend(validated_entity_terms)
    else:
        terms.extend(entity_terms)
    
    # Add action nodes (prepend with entity if available)
    action_nodes = kg.get('action_nodes', [])
    if main_entity:
        entity_action_terms = [f"{main_entity} {action}" for action in action_nodes]
        terms.extend(entity_action_terms)
    else:
        terms.extend(action_nodes)
    
    # Add related concepts (should already contain entity name per prompt)
    terms.extend(kg.get('related_concepts', []))
    
    # Add search terms WITHOUT entity as backup (lower priority)
    terms.extend(kg.get('search_terms_without_entity', []))
    
    # Remove duplicates and empty strings
    terms = [t.strip() for t in terms if t and t.strip()]
    unique_terms = list(dict.fromkeys(terms))  # Preserve order while removing duplicates
    
    # Log entity coverage
    if main_entity:
        entity_count = sum(1 for term in unique_terms if main_entity.lower() in term.lower())
        print(f"  Entity Coverage: {entity_count}/{len(unique_terms)} terms contain '{main_entity}'")
    
    return unique_terms

def fetch_articles_for_terms_parallel(search_terms: List[str], max_records: int = 100) -> pd.DataFrame:
    """Fetch articles using parallel processing for better efficiency"""
    def fetch_single_term(kw):
        try:
            df = get_and_save_gdelt_articles(query=kw, max_records=max_records, output_file=None)
            if not df.empty:
                df = df.copy()
                df['keyword'] = kw
                return df
        except Exception as e:
            print(f"Error fetching '{kw}': {e}")
            # Check for proxy-related errors
            if "proxy" in str(e).lower() or "connection" in str(e).lower():
                print(f"  This might be a proxy issue - testing connection...")
                test_proxy_connection()
        return pd.DataFrame()
    
    print(f"Fetching articles for {len(search_terms)} terms in parallel...")
    
    dfs = []
    # Use ThreadPoolExecutor for parallel API calls
    with ThreadPoolExecutor(max_workers=5) as executor:
        future_to_term = {executor.submit(fetch_single_term, term): term for term in search_terms}
        
        for future in as_completed(future_to_term):
            term = future_to_term[future]
            try:
                df = future.result(timeout=60)
                if not df.empty:
                    dfs.append(df)
                    print(f"  âœ“ {term}: {len(df)} articles")
                else:
                    print(f"  âœ— {term}: no articles")
            except Exception as e:
                print(f"  âœ— {term}: {e}")
    
    if not dfs:
        return pd.DataFrame()
    
    combined = pd.concat(dfs, ignore_index=True).drop_duplicates(subset=['url'], keep='first')
    print(f"Combined {len(combined)} unique articles from {len(dfs)} successful queries")
    return combined

def deduplicate_by_title_and_reputation(df: pd.DataFrame, similarity_threshold: float = 85) -> pd.DataFrame:
    """Deduplicate articles by fuzzy title matching, keeping ones from better domains"""
    if df.empty or 'title' not in df.columns:
        return df
    
    # Define domain reputation scores (higher is better)
    domain_scores = {
        'reuters.com': 100, 'bloomberg.com': 95, 'wsj.com': 95, 'ft.com': 95,
        'apnews.com': 90, 'bbc.com': 90, 'cnn.com': 85, 'techcrunch.com': 85,
        'theverge.com': 80, 'venturebeat.com': 80, 'axios.com': 80,
        'yahoo.com': 70, 'cnbc.com': 75, 'forbes.com': 75, 'fortune.com': 75
    }
    
    df_copy = df.copy().reset_index(drop=True)
    
    # Add domain reputation score
    df_copy['domain_score'] = df_copy['domain'].map(domain_scores).fillna(50)
    
    # Sort by domain score (best first) to prioritize keeping high-quality sources
    df_copy = df_copy.sort_values(['domain_score', 'Date'], ascending=[False, False])
    
    to_remove = set()
    
    for i, row1 in df_copy.iterrows():
        if i in to_remove:
            continue
            
        title1 = row1['title'] if pd.notna(row1['title']) else ""
        
        for j, row2 in df_copy.iloc[i+1:].iterrows():
            if j in to_remove:
                continue
                
            title2 = row2['title'] if pd.notna(row2['title']) else ""
            
            if fuzz.ratio(title1, title2) >= similarity_threshold:
                to_remove.add(j)  # Remove the lower-scored duplicate
    
    result = df_copy.drop(index=list(to_remove)).reset_index(drop=True)
    print(f"  Removed {len(to_remove)} duplicates, kept {len(result)} articles")
    return result.drop('domain_score', axis=1)

def remove_exact_duplicates_only(df: pd.DataFrame) -> pd.DataFrame:
    """Remove only exact title duplicates, keeping higher reputation domains"""
    if df.empty or 'title' not in df.columns:
        return df
    
    # Simple exact duplicate removal by title
    df_copy = df.copy()
    
    # Add domain scores for tie-breaking
    domain_scores = {
        'techcrunch.com': 85, 'reuters.com': 100, 'bloomberg.com': 95,
        'biztoc.com': 60, 'cursor.com': 90, 'webrazzi.com': 55
    }
    df_copy['domain_score'] = df_copy['domain'].map(domain_scores).fillna(50)
    
    # Sort by domain score and keep first (highest score) for exact duplicates
    df_dedup = df_copy.sort_values('domain_score', ascending=False).drop_duplicates(
        subset=['title'], keep='first'
    ).drop('domain_score', axis=1)
    
    removed_count = len(df_copy) - len(df_dedup)
    if removed_count > 0:
        print(f"  Removed {removed_count} exact duplicates")
    
    return df_dedup.reset_index(drop=True)

def format_dates(df: pd.DataFrame) -> pd.DataFrame:
    """Format dates consistently"""
    if df.empty or 'Date' not in df.columns:
        return df
    
    df_copy = df.copy()
    df_copy['Date'] = pd.to_datetime(df_copy['Date'], errors='coerce').dt.strftime('%Y-%m-%d')
    return df_copy

def translate_headline_to_english(headline: str, source_language: str) -> str:
    """Translate non-English headlines to English using BlackRock LLM"""
    if not headline or pd.isna(headline) or source_language.lower() == 'english':
        return headline

    prompt = f"""
    Translate this {source_language} headline to English:
    
    Original: "{headline}"
    
    Provide only the English translation. Keep the meaning and context accurate.
    If it's already in English or unclear, return the original text.
    """

    system_prompt = "You are a professional translator. Translate news headlines accurately while preserving meaning and context."

    try:
        # Use structured output for translation
        translation_schema = {
            "type": "object",
            "properties": {"translated_text": {"type": "string"}},
            "required": ["translated_text"],
            "additionalProperties": False
        }
        
        result = llm_client.generate_structured(prompt, translation_schema, system_prompt, temperature=0.1, max_tokens=200)
        return result.get('translated_text', headline).strip() if result else headline
            
    except Exception as e:
        print(f"Translation error for '{headline[:30]}...': {e}")
        return headline
    
def translate_headlines_batch(df: pd.DataFrame, batch_size: int = 10) -> pd.DataFrame:
    """Translate non-English headlines in batches for efficiency"""
    if df.empty or 'title' not in df.columns or 'language' not in df.columns:
        return df
    
    df_copy = df.copy()
    
    # Find non-English articles
    non_english_mask = (df_copy['language'].str.lower() != 'english') & df_copy['title'].notna()
    non_english_indices = df_copy[non_english_mask].index.tolist()
    
    if not non_english_indices:
        print("  All headlines are already in English")
        # Add english_title column with original titles
        df_copy['english_title'] = df_copy['title']
        return df_copy
    
    print(f"  Translating {len(non_english_indices)} non-English headlines...")
    
    # Initialize english_title column with original titles
    df_copy['english_title'] = df_copy['title']
    
    # Process non-English headlines in batches
    for i in range(0, len(non_english_indices), batch_size):
        batch_end = min(i + batch_size, len(non_english_indices))
        batch_indices = non_english_indices[i:batch_end]
        
        # Use ThreadPoolExecutor for parallel translation
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for idx in batch_indices:
                headline = df_copy.at[idx, 'title']
                language = df_copy.at[idx, 'language']
                future = executor.submit(translate_headline_to_english, headline, language)
                futures.append((idx, future))
            
            # Collect results
            for idx, future in futures:
                try:
                    translated = future.result(timeout=30)
                    df_copy.at[idx, 'english_title'] = translated
                except Exception as e:
                    print(f"    Translation failed for index {idx}: {e}")
                    # Keep original title if translation fails
                    df_copy.at[idx, 'english_title'] = df_copy.at[idx, 'title']
        
        # Progress update
        print(f"    Translated {batch_end}/{len(non_english_indices)} headlines")
        
        # Small delay to avoid rate limits
        if batch_end < len(non_english_indices):
            time.sleep(0.3)
    
    return df_copy

def judge_headline_relevance(headline: str, query: str) -> Dict:
    """Use BlackRock LLM to judge the relevance of a headline to the query"""
    schema = {
        "type": "object",
        "properties": {
            "relevance_score": {"type": "number"},
            "reasoning": {"type": "string"},
            "key_matches": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["relevance_score", "reasoning", "key_matches"],
        "additionalProperties": False
    }

    prompt = f"""
    Score headline relevance to query (0.0-1.0):
    
    QUERY: "{query}"
    HEADLINE: "{headline}"
    
    SCORING CRITERIA:
    1.0: Perfect match (same entity + same event type)
    0.8-0.9: Same entity, related event OR same event, related entity  
    0.6-0.7: Same industry/sector with relevant context
    0.4-0.5: Tangentially related (competitor, adjacent market)
    0.1-0.3: Weak connection (same industry only)
    0.0: Completely unrelated
    
    OUTPUT:
    - relevance_score: Single decimal (0.0-1.0)
    - reasoning: One sentence explanation
    - key_matches: List of specific matching elements
    """

    system_prompt = "You are a financial news analyst. Evaluate relevance precisely using the scoring rubric. Be consistent and objective."

    try:
        result = llm_client.generate_structured(prompt, schema, system_prompt, temperature=0.1, max_tokens=300)
        if result:
            score = min(max(float(result.get('relevance_score', 0.0)), 0.0), 1.0)
            return {
                "relevance_score": score, 
                "reasoning": result.get('reasoning', ''), 
                "key_matches": result.get('key_matches', [])
            }
        else:
            return {"relevance_score": 0.0, "reasoning": "Error in evaluation", "key_matches": []}
        
    except Exception as e:
        print(f"Error judging relevance: {e}")
        return {"relevance_score": 0.0, "reasoning": "Error in evaluation", "key_matches": []}
    
def add_llm_relevance_scores_batch(df: pd.DataFrame, query: str, batch_size: int = 5) -> pd.DataFrame:
    """Add LLM relevance scores using batch processing with English titles"""
    if df.empty or 'english_title' not in df.columns:
        return df
    
    df_copy = df.copy().reset_index(drop=True)
    scores, reasons, matches = [], [], []
    
    print(f"  Processing {len(df_copy)} articles in batches of {batch_size}...")
    
    # Process in batches to avoid rate limits and improve efficiency
    for i in range(0, len(df_copy), batch_size):
        batch_end = min(i + batch_size, len(df_copy))
        batch = df_copy.iloc[i:batch_end]
        
        batch_results = []
        
        # Use ThreadPoolExecutor for parallel processing within batch
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for _, row in batch.iterrows():
                # Use English title for relevance scoring
                english_headline = row['english_title'] if pd.notna(row['english_title']) else ""
                future = executor.submit(judge_headline_relevance, english_headline, query)
                futures.append(future)
            
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=30)
                    batch_results.append(result)
                except Exception as e:
                    print(f"  Batch processing error: {e}")
                    batch_results.append({"relevance_score": 0.0, "reasoning": "Processing error", "key_matches": []})
        
        # Add batch results
        for result in batch_results:
            scores.append(round(result['relevance_score'], 3))
            reasons.append(result['reasoning'])
            matches.append(', '.join(result['key_matches']))
        
        # Progress update
        print(f"    Processed {batch_end}/{len(df_copy)} articles")
        
        # Small delay to avoid rate limits
        if batch_end < len(df_copy):
            time.sleep(0.5)
    
    df_copy['llm_relevance_score'] = scores
    df_copy['llm_reasoning'] = reasons
    df_copy['llm_key_matches'] = matches
    
    return df_copy

def compute_combined_scores(df: pd.DataFrame) -> pd.DataFrame:
    """Compute combined scores - simplified since scores are already 0-1"""
    df_copy = df.copy()
    
    # Simple weighted average (scores already normalized 0-1)
    df_copy['score_avg'] = (df_copy['semantic_similarity'] + df_copy['llm_relevance_score']) / 2
    df_copy['score_prod'] = df_copy['semantic_similarity'] * df_copy['llm_relevance_score']
    df_copy['score_geo'] = np.sqrt(df_copy['score_prod'])
    
    return df_copy

def split_articles_by_subject(
    articles_df: pd.DataFrame,
    subject: str,
    query: str,
    remove_duplicates: bool = True,
    remove_exact_only: bool = False  # New parameter
) -> pd.DataFrame:
    """Process articles that mention the subject with full pipeline"""
    if articles_df.empty:
        return pd.DataFrame()

    df = articles_df.copy()
    tokenized = [t.lower().split() if pd.notna(t) else [] for t in df['title']]
    bm25 = BM25Okapi(tokenized)
    df['bm25_score'] = bm25.get_scores(subject.lower().split())

    cols = ['title', 'Date', 'domain', 'url', 'language', 'keyword']
    df_with = df[df['bm25_score'] > 0][cols].reset_index(drop=True)

    print(f"\nProcessing {len(df_with)} subject-related articles...")
    
    # Conditional deduplication
    if remove_duplicates:
        df_with = deduplicate_by_title_and_reputation(df_with)
    elif remove_exact_only:
        df_with = remove_exact_duplicates_only(df_with)  # Exact matching only
    else:
        print("  Skipping all deduplication...")
    
    df_with = format_dates(df_with)

    if not df_with.empty:
        # Full pipeline for subject-related articles
        print("Translating non-English headlines...")
        df_with = translate_headlines_batch(df_with, batch_size=10)
        
        print("Computing semantic similarity...")
        df_with = calculate_semantic_similarity_efficient(df_with, query)
        
        print("Evaluating LLM relevance...")
        df_with = add_llm_relevance_scores_batch(df_with, query, batch_size=5)
        
        df_with = compute_combined_scores(df_with)
        df_with = df_with.sort_values('score_geo', ascending=False).reset_index(drop=True)
    
    return df_with

def add_news_id_and_filter(df: pd.DataFrame) -> pd.DataFrame:
    """Add unique news_id and filter articles with score_geo > 0"""
    if df.empty:
        return df
    
    df_copy = df.copy()
    
    # Add unique news_id for each article
    df_copy['news_id'] = [str(uuid.uuid4())[:8] for _ in range(len(df_copy))]
    
    # Filter articles with score_geo > 0
    # df_filtered = df_copy[df_copy['score_geo'] > 0].reset_index(drop=True)
    
    # print(f"  Filtered to {len(df_filtered)} articles with score_geo > 0")
    return df_copy

def generate_summary_with_llm(df: pd.DataFrame, query: str) -> str:
    """Generate a comprehensive summary using filtered articles with dynamic filtering"""
    if df.empty:
        return "No relevant articles found to summarize."
    
    # Import re at function level to avoid scope issues
    import re
    
    # DYNAMIC FILTERING: Apply score_geo > 0 filter only if we have more than 5 articles
    if len(df) > 5:
        print(f"ðŸ“Š {len(df)} articles available - applying score_geo > 0 filter")
        df_relevant = df[df['score_geo'] > 0].copy()
        
        if df_relevant.empty:
            print("âš ï¸ No articles with score_geo > 0, using all available articles")
            df_relevant = df.copy()
        else:
            print(f"âœ… Filtered to {len(df_relevant)} articles with score_geo > 0")
    else:
        print(f"ðŸ“Š Only {len(df)} articles available - using all articles for summary generation")
        df_relevant = df.copy()
    
    # Limit to top articles but ensure quality (max 15 for processing efficiency)
    df_top = df_relevant.nlargest(15, 'score_geo') if len(df_relevant) > 15 else df_relevant
    
    print(f"ðŸ“ Using {len(df_top)} articles for summary generation")
    
    # Enhanced article data preparation with FULL CONTENT
    articles_text = ""
    news_id_mapping = {}
    
    for idx, row in df_top.iterrows():
        news_id = row['news_id']
        news_id_mapping[news_id] = {
            'title': row['english_title'],
            'domain': row['domain'],
            'date': row['Date'],
            'url': row.get('url', '#'),
            'relevance_score': row['score_geo'],
            'reasoning': row.get('llm_reasoning', '')
        }
        
        # Clean title and include relevance context
        clean_title = str(row['english_title']).replace('"', "'").replace('\n', ' ')
        
        articles_text += f"""
        Article {idx+1} [ID: {news_id}]:
        Title: {clean_title}
        Date: {row['Date']}
        Source: {row['domain']}
        Relevance Score: {row['score_geo']:.3f}
        ---
        """
    
    summary_schema = {
        "type": "object",
        "properties": {
            "answer": {"type": "string"},
            "key_findings": {"type": "array", "items": {"type": "string"}},
            "timeline": {"type": "array", "items": {"type": "string"}},
            "sources_count": {"type": "number"},
            "confidence_level": {"type": "string"}
        },
        "required": ["answer", "key_findings", "timeline", "sources_count", "confidence_level"],
        "additionalProperties": False
    }
    
    # Create list of available news_ids for the prompt
    available_ids = list(news_id_mapping.keys())
    
    # MUCH CLEANER AND MORE NATURAL PROMPT
    prompt = f"""
    Create a comprehensive news summary responding to the query: "{query}"
    
    Based on the following {len(df_top)} relevant articles:
    {articles_text}
    
    REQUIREMENTS:
    1. ANSWER: 2-3 paragraph overview answering the query directly. Include inline citations using [news_id] format where specific information comes from articles. Use these available news_ids: {available_ids}
    2. KEY_FINDINGS: 5-7 bullet points of main insights
    3. TIMELINE: Chronological sequence of events mentioned
    4. SOURCES_COUNT: {len(df_top)}
    5. CONFIDENCE_LEVEL: High/Medium/Low based on source quality and consistency
    
    CITATION REQUIREMENTS:
    - In the ANSWER section, add [news_id] citations after specific facts, quotes, or data points
    - Example: "Harvard endowment returned 8.1% [a1b2c3d4] outperforming benchmarks [b2c3d4e5]"
    - Be precise about which article each piece of information comes from
    
    FOCUS ON:
    - Direct answers to the query
    - Financial details (valuations, funding amounts, performance metrics)
    - Key stakeholders and institutions involved
    - Market context and implications
    - Recent developments and trends
    
    Be factual, concise, and cite specific details from the articles using the news_id format.
    """
    
    try:
        print(f"ðŸ”„ Generating summary with {llm_client.provider} model: {llm_client.model}")
        print(f"ðŸ“Š Articles in prompt: {len(df_top)}")
        print(f"ðŸ“ Prompt length: {len(prompt)} characters")
        
        # Use the BlackRock LLM client's structured generation method
        result = llm_client.generate_structured(
            prompt, 
            summary_schema, 
            system_prompt="You are a financial news analyst. Create comprehensive, well-cited summaries that directly answer user queries with specific details from the provided articles.",
            temperature=0.1,
            max_tokens=3000
        )
        
        if not result:
            print("âŒ No result from structured generation")
            return create_conservative_fallback_summary(df, query, news_id_mapping, df_top)
        
        print("âœ… Summary generated successfully")
        print(f"ðŸŽ¯ Answer length: {len(result.get('answer', ''))} characters")
        print(f"ðŸ” Key findings count: {len(result.get('key_findings', []))}")
        print(f"ðŸ“… Timeline events count: {len(result.get('timeline', []))}")
        print(f"ðŸŽ¯ Confidence level: {result.get('confidence_level', 'Unknown')}")
        
        # Enhanced summary formatting with ONLY CITED SOURCES
        summary = f"""
ðŸ“Š NEWS SUMMARY: {query}
{'='*60}

ðŸŽ¯ ANSWER:
{result['answer']}

ðŸ” KEY FINDINGS:
"""
        for i, finding in enumerate(result['key_findings'], 1):
            summary += f"{i}. {finding}\n"
        
        summary += f"""
ðŸ“… TIMELINE:
"""
        for event in result['timeline']:
            summary += f"â€¢ {event}\n"
        
        # ONLY SHOW ACTUALLY CITED SOURCES
        cited_ids = re.findall(r'\[([a-f0-9]{8})\]', result['answer'])
        print(f"ðŸ”— Found {len(cited_ids)} citations in answer: {cited_ids}")
        
        if cited_ids:
            summary += f"""
ðŸ“‹ CITED SOURCES ({len(set(cited_ids))} unique sources):
"""
            # Remove duplicates while preserving order
            unique_cited_ids = list(dict.fromkeys(cited_ids))
            
            for i, news_id in enumerate(unique_cited_ids, 1):
                if news_id in news_id_mapping:
                    info = news_id_mapping[news_id]
                    summary += f"{i}. [{info['title'][:100]}...]({info['url']}) ({info['domain']}, {info['date']}) - Relevance: {info['relevance_score']:.3f}\n"
                else:
                    print(f"âš ï¸ Citation {news_id} not found in news_id_mapping")
        else:
            print("â„¹ï¸ No citations found in answer, showing top sources")
            summary += f"""
ðŸ“‹ KEY SOURCES (Top 5 by relevance):
"""
            # If no citations, show top 5 sources as reference
            for i, (news_id, info) in enumerate(list(news_id_mapping.items())[:5], 1):
                summary += f"{i}. [{info['title'][:80]}...]({info['url']}) ({info['domain']}, {info['date']}) - Score: {info['relevance_score']:.3f}\n"
        
        # ADAPTIVE METADATA based on filtering applied
        filtering_note = ""
        if len(df) > 5 and len(df_relevant) < len(df):
            filtering_note = f"Applied score_geo > 0 filter ({len(df_relevant)}/{len(df)} articles)"
        elif len(df) <= 5:
            filtering_note = f"Used all available articles (limited dataset: {len(df)} articles)"
        else:
            filtering_note = f"Used all articles (no filtering needed: {len(df)} articles)"
        
        summary += f"""
ðŸ“ˆ ANALYSIS METADATA:
â€¢ Query: {query}
â€¢ Data Processing: {filtering_note}
â€¢ Articles Analyzed: {len(df_top)} articles for summary generation
â€¢ Total Articles Available: {len(df)} articles
â€¢ Confidence Level: {result['confidence_level']}
â€¢ Date Range: {df['Date'].min()} to {df['Date'].max()}
â€¢ Average Relevance Score: {df_top['score_geo'].mean():.3f}
â€¢ Sources Actually Cited: {len(set(cited_ids))} unique citations
â€¢ LLM Provider: {llm_client.provider} ({llm_client.model})
â€¢ Proxy Status: {'Active' if os.environ.get('HTTPS_PROXY') else 'Not configured'}
"""
        
        print("âœ… Summary generation completed successfully")
        return summary
        
    except Exception as e:
        print(f"âŒ Error during API call or processing: {e}")
        print(f"ðŸ” Error type: {type(e).__name__}")
        print(f"ðŸ” Error details: {str(e)}")
        
        # Check for proxy errors
        if "proxy" in str(e).lower() or "connection" in str(e).lower():
            print("ðŸ” This appears to be a proxy-related error")
            test_proxy_connection()
        
        print("ðŸ”„ Falling back to conservative summary...")
        return create_conservative_fallback_summary(df, query, news_id_mapping, df_top)

def create_conservative_fallback_summary(df: pd.DataFrame, query: str, news_id_mapping: dict, df_top: pd.DataFrame) -> str:
    """Conservative fallback that only reports what's in headlines - completely dynamic"""
    
    # Extract query terms and analyze query type dynamically
    query_terms = [term.strip().lower() for term in query.split() if len(term.strip()) > 2]
    query_lower = query.lower()
    
    # Dynamically determine focus area based on query terms
    focus_keywords = {
        'financial': ['endowment', 'fund', 'investment', 'return', 'performance', 'financial', 'revenue', 'profit'],
        'funding': ['fundraising', 'funding', 'investment', 'round', 'capital', 'venture', 'series'],
        'product': ['product', 'launch', 'release', 'feature', 'software', 'platform', 'tool'],
        'corporate': ['acquisition', 'merger', 'partnership', 'deal', 'agreement'],
        'academic': ['research', 'study', 'university', 'college', 'education', 'academic'],
        'legal': ['lawsuit', 'court', 'legal', 'ruling', 'judge', 'case'],
        'governance': ['board', 'ceo', 'management', 'leadership', 'executive']
    }
    
    # Determine focus area dynamically
    focus_area = "business activities and developments"  # default
    focus_terms = query_terms.copy()
    
    for category, keywords in focus_keywords.items():
        matches = sum(1 for term in query_terms if any(keyword in term or term in keyword for keyword in keywords))
        if matches > 0:
            if category == 'financial':
                focus_area = f"{' '.join([t for t in query_terms if any(k in t for k in keywords)])} performance and financial management"
                focus_terms = keywords
            elif category == 'funding':
                focus_area = f"{' '.join([t for t in query_terms if any(k in t for k in keywords)])} activities and investment rounds"
                focus_terms = keywords
            elif category == 'product':
                focus_area = f"{' '.join([t for t in query_terms if any(k in t for k in keywords)])} development and market activities"
                focus_terms = keywords
            else:
                focus_area = f"{category} activities and developments"
                focus_terms = keywords
            break
    
    # Analyze headlines dynamically
    headlines = [str(row['english_title']) if pd.notna(row['english_title']) else str(row.get('title', '')) for _, row in df_top.iterrows()]
    
    # Count query-specific matches dynamically
    specific_matches = 0
    headline_matches = []
    
    for i, headline in enumerate(headlines):
        headline_lower = headline.lower()
        matches_found = []
        
        for term in focus_terms:
            if term in headline_lower:
                matches_found.append(term)
        
        if matches_found:
            specific_matches += 1
            headline_matches.append((i, headline, matches_found))
    
    # Dynamic date range calculation
    date_range_days = 0
    min_date = df['Date'].min()
    max_date = df['Date'].max()
    
    try:
        date_range_days = (pd.to_datetime(max_date) - pd.to_datetime(min_date)).days
    except:
        date_range_days = 0
    
    # Dynamic entity extraction from query
    main_entity = ""
    for term in query.split():
        if term and len(term) > 2 and term not in ['the', 'and', 'for', 'with', 'about']:
            main_entity = term
            break
    
    if not main_entity:
        main_entity = "the subject"
    
    fallback_summary = f"""
ðŸ“Š NEWS SUMMARY: {query}
{'='*60}

ðŸŽ¯ ANALYSIS:
Based on analysis of {len(df)} news headlines, specific information about "{query}" is limited in the available coverage. 

The search identified {len(df_top)} articles with some relevance to {main_entity}, but only {specific_matches} headlines contain terms directly related to {focus_area}.

Most available coverage focuses on general news rather than specific details about {focus_area}. The headlines suggest ongoing developments but lack detailed information about {query}.

ðŸ” KEY OBSERVATIONS:
1. Limited specific coverage of {focus_area} in available headlines
2. Coverage period spans {date_range_days} days from {min_date} to {max_date}
3. {specific_matches} of {len(df_top)} top articles contain query-related terms in headlines
4. Most coverage focuses on other topics rather than {focus_area}
5. Specific metrics, performance data, or detailed information not visible in headline analysis
"""
    
    # Add matching headlines if any found
    if headline_matches:
        fallback_summary += f"""
6. Headlines containing relevant terms: {len(headline_matches)} articles mention {', '.join(set([match for _, _, matches in headline_matches for match in matches]))}
"""
    
    fallback_summary += f"""

ðŸ“… HEADLINE-BASED TIMELINE:
"""
    
    # Show headlines chronologically
    for idx, row in df_top.head(5).iterrows():
        headline_preview = str(row['english_title'] if pd.notna(row['english_title']) else row.get('title', 'No title'))[:100]
        fallback_summary += f"â€¢ {row['Date']}: {headline_preview}... ({row['domain']})\n"
    
    fallback_summary += f"""

ðŸ“‹ MOST RELEVANT SOURCES:
"""
    
    # Show top sources with dynamic scoring
    for idx, row in df_top.head(5).iterrows():
        headline_preview = str(row['english_title'] if pd.notna(row['english_title']) else row.get('title', 'No title'))[:100]
        url = row.get('url', '#')
        score = row.get('score_geo', 0.0)
        fallback_summary += f"â€¢ [{headline_preview}...]({url}) ({row['domain']}, {row['Date']}) - Score: {score:.3f}\n"
    
    # Dynamic top sources calculation
    top_domains = df['domain'].value_counts().head(3).index.tolist()
    avg_relevance = df_top['score_geo'].mean() if len(df_top) > 0 else 0.0
    
    fallback_summary += f"""
ðŸ“ˆ ANALYSIS METADATA:
â€¢ Query: {query}
â€¢ Headlines Analyzed: {len(df)} total, {len(df_top)} most relevant
â€¢ Query-Specific Matches: {specific_matches} headlines
â€¢ Date Range: {min_date} to {max_date} ({date_range_days} days)
â€¢ Top Sources: {', '.join(top_domains) if top_domains else 'Various sources'}
â€¢ Average Relevance: {avg_relevance:.3f}
â€¢ Confidence Level: {'Medium' if specific_matches > len(df_top) * 0.3 else 'Low'} (headline-only analysis, {'specific data found' if specific_matches > 0 else 'limited specific data'})
â€¢ Data Limitation: Analysis based on headlines only - full article content needed for detailed information
â€¢ Focus Area: {focus_area}
â€¢ Main Entity: {main_entity}
â€¢ Search Terms Used: {len(query_terms)} terms derived from query
â€¢ Recommendation: {'Expand search terms' if specific_matches == 0 else 'Search specialized publications'} for {focus_area.replace(' activities and developments', '')} information
â€¢ Proxy Status: {'Active' if os.environ.get('HTTPS_PROXY') else 'Not configured'}
â€¢ LLM Provider: BlackRock API (gpt-4o)
"""
    
    return fallback_summary

# Update the main function to include filtering and summary
def main(query: str = "Cursor's latest fundraising", remove_duplicates: bool = True, remove_exact_only: bool = False, generate_summary: bool = True):
    print("Knowledge Graph Extraction")
    kg = extract_knowledge_graph_with_llm(query)
    terms = build_search_term_list(kg)
    print(f"Search Terms ({len(terms)}): {terms}")

    all_articles = fetch_articles_for_terms_parallel(terms, max_records=250)
    if all_articles.empty:
        print("No articles retrieved")
        return pd.DataFrame(), ""

    subject = kg['entities'][0]['text'] if kg.get('entities') else None
    if subject:
        print(f"Processing articles for subject '{subject}'...")
        df_processed = split_articles_by_subject(all_articles, subject, query, remove_duplicates, remove_exact_only)
        print(f"Final result: {len(df_processed)} relevant articles")
    else:
        print("No subject entity found - processing all articles")
        # Process all articles with basic pipeline
        if remove_duplicates:
            df_processed = deduplicate_by_title_and_reputation(all_articles)
        elif remove_exact_only:
            df_processed = remove_exact_duplicates_only(all_articles)
        else:
            df_processed = all_articles.copy()
            print("  Skipping deduplication...")
        df_processed = format_dates(df_processed)
        df_processed = calculate_semantic_similarity_efficient(df_processed, query)
    
    # Add news_id and filter by score_geo > 0
    if not df_processed.empty:
        df_filtered = add_news_id_and_filter(df_processed)
        
        # Generate summary if requested
        summary = ""
        if generate_summary and not df_filtered.empty:
            print("\nGenerating comprehensive summary...")
            summary = generate_summary_with_llm(df_filtered, query)
        
        return df_filtered, summary
    
    return pd.DataFrame(), ""


if __name__ == "__main__":
    # Get filtered results with news_id and summary
    df_results, summary = main("Cursor fundraising", remove_duplicates=False, remove_exact_only=True, generate_summary=True)

    if df_results is not None and not df_results.empty:
        display_cols = ['news_id', 'english_title', 'Date', 'domain', 'semantic_similarity', 'llm_relevance_score', 'score_geo']
        available_cols = [col for col in display_cols if col in df_results.columns]
        
        if summary:
            print("\n" + summary)
    else:
        print("No relevant articles found.")
