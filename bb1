import streamlit as st
import pandas as pd
import json
import time
from datetime import datetime
import io
import base64
import os

# Import the proxy setup and main analysis functions
# Note: Save your enhanced script as 'news_analysis.py' in the same directory
from news_analysis import (
    setup_corporate_proxy,
    test_proxy_connection,
    extract_knowledge_graph_with_llm,
    build_search_term_list,
    fetch_articles_for_terms_parallel,
    split_articles_by_subject,
    deduplicate_by_title_and_reputation,
    remove_exact_duplicates_only,
    format_dates,
    calculate_semantic_similarity_efficient,
    add_news_id_and_filter,
    generate_summary_with_llm,
    GenericLLMClient
)

# Initialize proxy configuration first
setup_corporate_proxy()

# Page configuration
st.set_page_config(
    page_title="News Analysis Dashboard",
    page_icon="üì∞",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
    <style>
    .main {
        padding-top: 2rem;
    }
    .stButton>button {
        width: 100%;
        background-color: #1f77b4;
        color: white;
        font-weight: bold;
    }
    .stButton>button:hover {
        background-color: #0056b3;
    }
    .summary-box {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin-top: 20px;
    }
    .metric-card {
        background-color: #ffffff;
        padding: 15px;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        margin-bottom: 10px;
    }
    .stExpander {
        background-color: #f8f9fa;
        border-radius: 10px;
    }
    </style>
    """, unsafe_allow_html=True)

# Initialize session state
if 'analysis_complete' not in st.session_state:
    st.session_state.analysis_complete = False
if 'df_results' not in st.session_state:
    st.session_state.df_results = pd.DataFrame()
if 'summary' not in st.session_state:
    st.session_state.summary = ""
if 'knowledge_graph' not in st.session_state:
    st.session_state.knowledge_graph = {}
if 'search_terms' not in st.session_state:
    st.session_state.search_terms = []
if 'llm_client' not in st.session_state:
    st.session_state.llm_client = None
if 'proxy_status' not in st.session_state:
    st.session_state.proxy_status = False

# Header
st.title("üì∞ Advanced News Analysis Dashboard")
st.markdown("**Powered by LLM-based relevance scoring and semantic analysis**")

# Sidebar configuration
with st.sidebar:
    st.header("‚öôÔ∏è Configuration")
    
    # Proxy status indicator
    proxy_configured = os.environ.get('HTTPS_PROXY', '') != ''
    if proxy_configured:
        st.success(f"‚úÖ Proxy: {os.environ.get('HTTPS_PROXY')}")
    else:
        st.warning("‚ö†Ô∏è No proxy configured")
    
    # Test proxy connection button
    if st.button("üîç Test Proxy Connection"):
        with st.spinner("Testing proxy..."):
            proxy_ok = test_proxy_connection()
            st.session_state.proxy_status = proxy_ok
            if proxy_ok:
                st.success("‚úÖ Proxy connection successful!")
            else:
                st.error("‚ùå Proxy connection failed!")
    
    st.markdown("---")
    
    # LLM Provider selection
    llm_provider = st.selectbox(
        "LLM Provider",
        ["cerebras", "openai", "groq", "gemini"],
        help="Select the LLM provider for analysis"
    )
    
    # Initialize LLM client when provider changes
    if st.session_state.llm_client is None or st.session_state.llm_client.provider != llm_provider:
        try:
            st.session_state.llm_client = GenericLLMClient(llm_provider)
            st.success(f"‚úÖ {llm_provider.title()} client initialized")
        except Exception as e:
            st.error(f"‚ùå Failed to initialize {llm_provider}: {str(e)}")
    
    # Processing options
    st.subheader("üìã Processing Options")
    
    dedup_option = st.radio(
        "Deduplication Method",
        ["No deduplication", "Exact matches only", "Fuzzy matching (85% threshold)"],
        help="Choose how to handle duplicate articles"
    )
    
    generate_summary = st.checkbox(
        "Generate AI Summary",
        value=True,
        help="Generate a comprehensive summary of the results"
    )
    
    max_records = st.slider(
        "Max records per search term",
        min_value=50,
        max_value=500,
        value=250,
        step=50,
        help="Maximum number of articles to fetch per search term"
    )
    
    # Advanced options
    with st.expander("üîß Advanced Options"):
        batch_size_translation = st.number_input(
            "Translation batch size",
            min_value=5,
            max_value=20,
            value=10,
            help="Number of headlines to translate in parallel"
        )
        
        batch_size_relevance = st.number_input(
            "Relevance scoring batch size",
            min_value=3,
            max_value=10,
            value=5,
            help="Number of articles to score in parallel"
        )
        
        top_articles_summary = st.number_input(
            "Top articles for summary",
            min_value=5,
            max_value=25,
            value=15,
            help="Number of top articles to use for summary generation"
        )

# Main content area
col1, col2 = st.columns([2, 1])

with col1:
    # Query input
    query = st.text_input(
        "üîç Enter your search query",
        placeholder="e.g., Harvard endowment performance",
        help="Enter a specific query about news you want to analyze"
    )

with col2:
    # Analyze button
    st.markdown("<br>", unsafe_allow_html=True)
    analyze_button = st.button("üöÄ Analyze News", type="primary")

# Analysis process
if analyze_button and query:
    if st.session_state.llm_client is None:
        st.error("‚ùå Please initialize an LLM provider first!")
    else:
        # Reset session state
        st.session_state.analysis_complete = False
        st.session_state.df_results = pd.DataFrame()
        st.session_state.summary = ""
        
        # Progress tracking
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        try:
            # Step 1: Knowledge Graph Extraction
            status_text.text("üß† Extracting knowledge graph...")
            progress_bar.progress(10)
            
            # Override the global llm_client with the session state one
            import news_analysis
            news_analysis.llm_client = st.session_state.llm_client
            
            kg = extract_knowledge_graph_with_llm(query)
            st.session_state.knowledge_graph = kg
            
            # Step 2: Build search terms
            status_text.text("üî§ Building search terms...")
            progress_bar.progress(20)
            
            terms = build_search_term_list(kg)
            st.session_state.search_terms = terms
            
            # Display extracted information
            with st.expander("üìä Knowledge Graph Analysis", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("Entities Identified")
                    if kg.get('entities'):
                        for entity in kg['entities']:
                            st.write(f"‚Ä¢ **{entity['text']}** ({entity['type']})")
                    else:
                        st.write("No entities identified")
                    
                    st.subheader("Action Nodes")
                    if kg.get('action_nodes'):
                        for node in kg['action_nodes']:
                            st.write(f"‚Ä¢ {node}")
                    else:
                        st.write("No action nodes identified")
                
                with col2:
                    st.subheader(f"Search Terms ({len(terms)})")
                    # Show first 10 terms
                    for i, term in enumerate(terms[:10]):
                        st.write(f"{i+1}. {term}")
                    if len(terms) > 10:
                        st.write(f"... and {len(terms)-10} more terms")
            
            # Step 3: Fetch articles
            status_text.text(f"üì• Fetching articles for {len(terms)} search terms...")
            progress_bar.progress(30)
            
            all_articles = fetch_articles_for_terms_parallel(terms, max_records=max_records)
            
            if all_articles.empty:
                st.error("‚ùå No articles found. Try modifying your query.")
                progress_bar.progress(100)
            else:
                st.success(f"‚úÖ Found {len(all_articles)} total articles")
                
                # Step 4: Process articles
                status_text.text("üîÑ Processing and deduplicating articles...")
                progress_bar.progress(50)
                
                # Determine deduplication method
                remove_duplicates = dedup_option == "Fuzzy matching (85% threshold)"
                remove_exact_only = dedup_option == "Exact matches only"
                
                # Get main subject entity
                subject = kg['entities'][0]['text'] if kg.get('entities') else None
                
                if subject:
                    df_processed = split_articles_by_subject(
                        all_articles, 
                        subject, 
                        query, 
                        remove_duplicates, 
                        remove_exact_only
                    )
                else:
                    # Basic processing without subject
                    if remove_duplicates:
                        df_processed = deduplicate_by_title_and_reputation(all_articles)
                    elif remove_exact_only:
                        df_processed = remove_exact_duplicates_only(all_articles)
                    else:
                        df_processed = all_articles.copy()
                    
                    df_processed = format_dates(df_processed)
                    df_processed = calculate_semantic_similarity_efficient(df_processed, query)
                
                # Step 5: Add IDs and filter
                status_text.text("üÜî Adding article IDs and filtering...")
                progress_bar.progress(70)
                
                if not df_processed.empty:
                    df_filtered = add_news_id_and_filter(df_processed)
                    st.session_state.df_results = df_filtered
                    
                    # Step 6: Generate summary
                    if generate_summary and not df_filtered.empty:
                        status_text.text("üìù Generating AI summary...")
                        progress_bar.progress(90)
                        
                        summary = generate_summary_with_llm(df_filtered, query)
                        st.session_state.summary = summary
                    
                    progress_bar.progress(100)
                    status_text.text("‚úÖ Analysis complete!")
                    st.session_state.analysis_complete = True
                    
        except Exception as e:
            st.error(f"‚ùå Error during analysis: {str(e)}")
            if "proxy" in str(e).lower() or "connection" in str(e).lower():
                st.warning("This might be a proxy-related issue. Check proxy settings in the sidebar.")
            progress_bar.progress(100)
            status_text.text("Analysis failed")

# Display results
if st.session_state.analysis_complete and not st.session_state.df_results.empty:
    
    # Results metrics
    st.markdown("---")
    st.header("üìä Analysis Results")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Articles", len(st.session_state.df_results))
    
    with col2:
        high_relevance = len(st.session_state.df_results[st.session_state.df_results['score_geo'] > 0.5])
        st.metric("High Relevance", high_relevance)
    
    with col3:
        avg_score = st.session_state.df_results['score_geo'].mean()
        st.metric("Avg. Relevance", f"{avg_score:.3f}")
    
    with col4:
        date_range = f"{st.session_state.df_results['Date'].min()} to {st.session_state.df_results['Date'].max()}"
        st.metric("Date Range", date_range)
    
    # Display summary
    if st.session_state.summary:
        st.markdown("---")
        st.header("üìù AI-Generated Summary")
        
        # Format and display summary in a nice box
        summary_container = st.container()
        with summary_container:
            # Split summary into sections for better display
            summary_lines = st.session_state.summary.split('\n')
            
            current_section = []
            for line in summary_lines:
                if line.startswith('üìä') or line.startswith('üéØ') or line.startswith('üîç') or \
                   line.startswith('üìÖ') or line.startswith('üìã') or line.startswith('üìà'):
                    if current_section:
                        st.markdown('\n'.join(current_section))
                    current_section = [line]
                    if line.startswith('üìä'):
                        st.markdown("### " + line)
                    elif any(line.startswith(emoji) for emoji in ['üéØ', 'üîç', 'üìÖ', 'üìã', 'üìà']):
                        st.markdown("#### " + line)
                else:
                    current_section.append(line)
            
            if current_section:
                st.markdown('\n'.join(current_section))
    
    # Articles table
    st.markdown("---")
    st.header("üì∞ Article Details")
    
    # Filtering options
    col1, col2, col3 = st.columns(3)
    
    with col1:
        min_score = st.slider(
            "Minimum relevance score",
            min_value=0.0,
            max_value=1.0,
            value=0.0,
            step=0.1
        )
    
    with col2:
        selected_domains = st.multiselect(
            "Filter by domain",
            options=st.session_state.df_results['domain'].unique(),
            default=[]
        )
    
    with col3:
        sort_by = st.selectbox(
            "Sort by",
            ["score_geo", "Date", "semantic_similarity", "llm_relevance_score"],
            index=0
        )
    
    # Apply filters
    filtered_df = st.session_state.df_results.copy()
    
    if min_score > 0:
        filtered_df = filtered_df[filtered_df['score_geo'] >= min_score]
    
    if selected_domains:
        filtered_df = filtered_df[filtered_df['domain'].isin(selected_domains)]
    
    # Sort
    filtered_df = filtered_df.sort_values(sort_by, ascending=False)
    
    # Display table
    if not filtered_df.empty:
        # Select columns to display
        display_cols = [
            'news_id', 'english_title', 'Date', 'domain', 
            'semantic_similarity', 'llm_relevance_score', 'score_geo',
            'llm_reasoning', 'url'
        ]
        
        # Check which columns exist
        available_cols = [col for col in display_cols if col in filtered_df.columns]
        
        # Create a styled dataframe
        st.dataframe(
            filtered_df[available_cols],
            use_container_width=True,
            height=400,
            hide_index=True,
            column_config={
                "news_id": st.column_config.TextColumn("ID", width="small"),
                "english_title": st.column_config.TextColumn("Title", width="large"),
                "Date": st.column_config.DateColumn("Date", width="small"),
                "domain": st.column_config.TextColumn("Source", width="small"),
                "semantic_similarity": st.column_config.NumberColumn("Semantic", format="%.3f", width="small"),
                "llm_relevance_score": st.column_config.NumberColumn("LLM Score", format="%.3f", width="small"),
                "score_geo": st.column_config.NumberColumn("Combined", format="%.3f", width="small"),
                "llm_reasoning": st.column_config.TextColumn("Reasoning", width="medium"),
                "url": st.column_config.LinkColumn("Link", width="small")
            }
        )
        
        # Download options
        st.markdown("---")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Download filtered results as CSV
            csv = filtered_df.to_csv(index=False)
            b64 = base64.b64encode(csv.encode()).decode()
            href = f'<a href="data:file/csv;base64,{b64}" download="news_analysis_results.csv">üì• Download Results (CSV)</a>'
            st.markdown(href, unsafe_allow_html=True)
        
        with col2:
            # Download summary as text
            if st.session_state.summary:
                summary_text = st.session_state.summary
                b64 = base64.b64encode(summary_text.encode()).decode()
                href = f'<a href="data:text/plain;base64,{b64}" download="news_analysis_summary.txt">üìÑ Download Summary (TXT)</a>'
                st.markdown(href, unsafe_allow_html=True)
        
        with col3:
            # Download full analysis as JSON
            analysis_data = {
                "query": query,
                "timestamp": datetime.now().isoformat(),
                "knowledge_graph": st.session_state.knowledge_graph,
                "search_terms": st.session_state.search_terms,
                "summary": st.session_state.summary,
                "articles": filtered_df.to_dict(orient='records'),
                "llm_provider": st.session_state.llm_client.provider if st.session_state.llm_client else "Unknown",
                "proxy_configured": proxy_configured
            }
            json_str = json.dumps(analysis_data, indent=2, default=str)
            b64 = base64.b64encode(json_str.encode()).decode()
            href = f'<a href="data:application/json;base64,{b64}" download="news_analysis_full.json">üìã Download Full Analysis (JSON)</a>'
            st.markdown(href, unsafe_allow_html=True)
    
    else:
        st.warning("No articles match the current filters")

# Footer
st.markdown("---")
st.markdown(
    f"""
    <div style='text-align: center; color: #666;'>
        <p>News Analysis Dashboard | Powered by GDELT, LLMs, and Semantic Analysis</p>
        <p>Built with Streamlit üéà | Proxy: {'‚úÖ Configured' if proxy_configured else '‚ùå Not configured'}</p>
    </div>
    """,
    unsafe_allow_html=True
)
