Thank you. Joy and I really appreciate the opportunity to walk through our code generation and execution case in Alturo. Also we want to thank Kashi, Brendon, Olga and Alex from the Aladdin AIAC to support and accommodate our forward thinking in AI solution risk, and we collectively want to achieve a better AI risk framework. Joy, do you want to kick off what Alturo is and what AI feature we're trying to achieve. 

When thinking about a AI solution risk, we first look across AI solution incidents across the industry and we think about how likely it would happen to us and how we evaluate and assess it. We come up with two simple thinking dimensions - mutalibity and network centrality - the idea came from how we think about infectious disease like biological agents to population risk. For mutalibity, this is where we want to understand how mutable the AI solution is. For example, if I ask you what colour of your T-shirt, if the AI answer is 'red'. The mutalibity is extremely small. But we all know that LLM operates on probablistic mechanims and even you put down the same prompt to Micrsoft copilit, you will very likely to get different answer very time. Moreover, if the solution only generates one programmaing language like Python and if we happen to master that language, that's great but what if we generate multiple languages like we don't have many people who master that language, it's hard to develop sandbox or quanrtaint. It's like English versus a islated language in remote island. The last mutalibity is if I ask you what colour of your T-shirt, if the AI answer is 'red', 

The most recent case is where a lone developer has reported that the AI agent at Replit has deleted all his database because the environment he operates is extremely to perform vibe coding as well as deployement - there's no multi-tier approval as well as enviroment isolation between development and production. This causes a lot of concerns where human can unknownly grant the agent on 
